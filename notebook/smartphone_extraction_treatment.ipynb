{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55dfbd49",
   "metadata": {},
   "source": [
    "## üì§Extra√ß√£o de Dados Vendas Celulares - Camada PRATAü•à\n",
    "- Carregar a base de dados em um DataFrame\n",
    "- Realizar os tratamentos, normaliza√ß√µes e convers√£o de dados\n",
    "- Salvar o DataFrame tratado na Camada PRATA (Silver)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f347ac",
   "metadata": {},
   "source": [
    "### üìöIMPORTs Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8263d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from decouple import Config, RepositoryEnv\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from datetime import datetime\n",
    "from pyspark.sql import DataFrame\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c9f71c",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏èCarregando Variaveis de Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a7dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "env_path = \".env\"\n",
    "config = Config(repository=RepositoryEnv(env_path))\n",
    "\n",
    "senha_banco_de_dados = config('DATA_BASE_PASSWORD')\n",
    "caminho_excel = config('CAMINHO_EXCEL')\n",
    "caminho_env = config('CAMINHO_ENV')\n",
    "#print(senha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1434d2c",
   "metadata": {},
   "source": [
    "### üìëCarregando [ EXCEL ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becefb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho completo para o arquivo\n",
    "caminho_arquivo = caminho_excel\n",
    "\n",
    "# Carrega o arquivo Excel em um DataFrame\n",
    "df_excel = pd.read_excel(caminho_arquivo)\n",
    "\n",
    "# Mostra o DF\n",
    "display(df_excel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106a0b67",
   "metadata": {},
   "source": [
    "### üõ†Ô∏èNormalizando coluna de `Valor` --> PANDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94c21ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---> Convertando dados com PANDAS\n",
    "\n",
    "# Fun√ß√£o para normalizar os valores\n",
    "def normalizar_valor(valor):\n",
    "    if pd.isna(valor):\n",
    "        return None\n",
    "    \n",
    "    # Converte para string\n",
    "    valor_str = str(valor)\n",
    "    \n",
    "    # Remove s√≠mbolo de moeda e espa√ßos\n",
    "    valor_str = re.sub(r'[^\\d,.-]', '', valor_str)\n",
    "    \n",
    "    # Substitui v√≠rgula por ponto, se for o caso\n",
    "    if ',' in valor_str and '.' not in valor_str:\n",
    "        valor_str = valor_str.replace(',', '.')\n",
    "    elif ',' in valor_str and '.' in valor_str:\n",
    "        # Caso raro tipo 1.234,56 ‚Üí 1234.56\n",
    "        valor_str = valor_str.replace('.', '').replace(',', '.')\n",
    "    \n",
    "    try:\n",
    "        # Converte para FLOAT\n",
    "        return float(valor_str)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# Aplica ao DataFrame\n",
    "df_excel['valor_normalizado'] = df_excel['Valor'].apply(normalizar_valor)\n",
    "\n",
    "# df final => EXCEL\n",
    "display(df_excel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58114106",
   "metadata": {},
   "source": [
    "### üîÉConvertendo DF de PANDAS para Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0750f1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß For√ßa o Spark a usar o Python do ambiente virtual\n",
    "os.environ[\"PYSPARK_PYTHON\"] = caminho_env\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = caminho_env\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
    "\n",
    "# üî• Cria a sess√£o Spark\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"ExcelToSpark\")\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "df_spark = spark.createDataFrame(df_excel)\n",
    "df_spark.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e1b07",
   "metadata": {},
   "source": [
    "### üîÉConvertendo Coluna de `Data da Venda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c335bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# fun√ß√£o Python para converter padr√£o de data\n",
    "def normalizar_data(valor):\n",
    "    if not valor:\n",
    "        return None\n",
    "    valor = str(valor).strip().replace(\"/\", \"-\")\n",
    "\n",
    "    formatos = [\"%d-%m-%Y\", \"%Y-%m-%d\", \"%m-%d-%Y\"]\n",
    "    for f in formatos:\n",
    "        try:\n",
    "            return datetime.strptime(valor, f).strftime(\"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return None  # se nenhum formato bater\n",
    "\n",
    "# registra a UDF\n",
    "normalizar_data_udf = F.udf(normalizar_data, StringType())\n",
    "\n",
    "# aplica no DataFrame Spark\n",
    "df_spark = df_spark.withColumn(\n",
    "    \"data_venda_normalizada\",\n",
    "    normalizar_data_udf(F.col(\"Data da Venda\"))\n",
    ")\n",
    "\n",
    "# converte para tipo date\n",
    "df_spark = df_spark.withColumn(\n",
    "    \"data_venda_normalizada\",\n",
    "    F.to_date(\"data_venda_normalizada\", \"yyyy-MM-dd\")\n",
    ")\n",
    "\n",
    "# mostra o resultado final\n",
    "df_spark.select(\"Data da Venda\", \"data_venda_normalizada\").show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75e1932",
   "metadata": {},
   "source": [
    "### üõ†Ô∏èNormalizando dados da coluna Nome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681b73b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_nome(nome: str) -> str:\n",
    "    if not nome:\n",
    "        return None\n",
    "    \n",
    "    nome = nome.strip().lower()\n",
    "\n",
    "    # Corrige casos comuns de nomes juntos\n",
    "    nome = re.sub(r'(\\d+)', r' \\1', nome)  # separa n√∫meros (ex: iphone15pro ‚Üí iphone 15pro)\n",
    "    nome = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', nome)  # separa letras coladas\n",
    "    \n",
    "    # Aplica capitaliza√ß√£o inicial\n",
    "    nome_formatado = \" \".join(word.capitalize() for word in nome.split())\n",
    "\n",
    "    # Mant√©m siglas espec√≠ficas em mai√∫sculo\n",
    "    siglas = {\"VI\", \"GT\", \"PRO\", \"ULTRA\", \"ROG\"}\n",
    "    palavras = []\n",
    "    for word in nome_formatado.split():\n",
    "        if word.upper() in siglas:\n",
    "            palavras.append(word.upper())\n",
    "        else:\n",
    "            palavras.append(word)\n",
    "    \n",
    "    # Corrige \"Iphone\" para \"iPhone\"\n",
    "    resultado = \" \".join(palavras)\n",
    "    resultado = re.sub(r'\\bIphone\\b', 'iPhone', resultado)\n",
    "\n",
    "    return resultado.strip()\n",
    "\n",
    "# Registrar como fun√ß√£o UDF no Spark\n",
    "udf_normalizar_nome = F.udf(normalizar_nome, StringType())\n",
    "\n",
    "# Aplicar a fun√ß√£o √† coluna \"Nome\"\n",
    "df_spark = df_spark.withColumn(\"Nome_Normalizado\", udf_normalizar_nome(F.col(\"Nome\")))\n",
    "\n",
    "# Exibir resultado\n",
    "df_spark.select(\"Nome\", \"Nome_Normalizado\").show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8956b3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5359749e",
   "metadata": {},
   "source": [
    "### üßπTirando colunas Indesejadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd658b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remover_colunas(df: DataFrame, colunas_para_remover: list) -> DataFrame:\n",
    "    colunas_existentes = [col for col in colunas_para_remover if col in df.columns]\n",
    "\n",
    "    if not colunas_existentes:\n",
    "        print(\"‚ö†Ô∏è Nenhuma das colunas informadas existe no DataFrame.\")\n",
    "        return df\n",
    "\n",
    "    print(f\"üßπ Removendo colunas: {colunas_existentes}\")\n",
    "    return df.drop(*colunas_existentes)\n",
    "\n",
    "# Lista de colunas para excluir\n",
    "colunas_excluir = [\n",
    "    \"Nome\", \n",
    "    \"Valor\", \n",
    "    \"Data da Venda\"\n",
    "]\n",
    "\n",
    "df_spark = remover_colunas(df_spark, colunas_excluir)\n",
    "\n",
    "df_spark.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738af905",
   "metadata": {},
   "source": [
    "### üîÑRenomeando nome das colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ab92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def renomear_colunas(df: DataFrame, mapeamento: dict) -> DataFrame:\n",
    "    df_renomeado = df\n",
    "    colunas_existentes = df.columns\n",
    "\n",
    "    for antiga, nova in mapeamento.items():\n",
    "        if antiga in colunas_existentes:\n",
    "            df_renomeado = df_renomeado.withColumnRenamed(antiga, nova)\n",
    "            print(f\"üîÅ Coluna renomeada: '{antiga}' ‚Üí '{nova}'\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Coluna '{antiga}' n√£o encontrada ‚Äî ignorada.\")\n",
    "\n",
    "    return df_renomeado\n",
    "\n",
    "# Chamando fun√ß√£o para Renomear\n",
    "mapeamento_colunas = {\n",
    "    \"Quantidade Vendida\": \"quantity_sold\",\n",
    "    \"valor_normalizado\": \"price\",\n",
    "    \"data_venda_normalizada\": \"sale_date\",\n",
    "    \"Nome_Normalizado\": \"name\"\n",
    "}\n",
    "\n",
    "df_spark = renomear_colunas(df_spark, mapeamento_colunas)\n",
    "\n",
    "df_spark.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdcede2",
   "metadata": {},
   "source": [
    "### üìåAplicando nova ordem das colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b02c357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reordenar_colunas(df: DataFrame, nova_ordem: list) -> DataFrame:\n",
    "    colunas_existentes = df.columns\n",
    "\n",
    "    # Garante que s√≥ use colunas que realmente existem no df\n",
    "    colunas_validas = [c for c in nova_ordem if c in colunas_existentes]\n",
    "\n",
    "    # Adiciona ao final as colunas que n√£o estavam na lista (para n√£o perder nenhuma)\n",
    "    colunas_restantes = [c for c in colunas_existentes if c not in colunas_validas]\n",
    "\n",
    "    ordem_final = colunas_validas + colunas_restantes\n",
    "\n",
    "    print(\"üìã Nova ordem de colunas:\")\n",
    "    print(ordem_final)\n",
    "\n",
    "    return df.select(ordem_final)\n",
    "\n",
    "nova_ordem = [\n",
    "    \"name\",\n",
    "    \"price\",\n",
    "    \"quantity_sold\",\n",
    "    \"sale_date\",\n",
    "]\n",
    "\n",
    "df_spark = reordenar_colunas(df_spark, nova_ordem)\n",
    "\n",
    "df_spark.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e15c64a",
   "metadata": {},
   "source": [
    "### üíæSalvando DF na camada [ PRATA ] --> Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf43603b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para n√£o sobrecarregar o SPARK\n",
    "df_sample = df_spark.limit(1000).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = df_sample\n",
    "\n",
    "# cria a engine de conex√£o com o PostgreSQL\n",
    "engine = create_engine(\n",
    "    'postgresql+psycopg2://kevinsoffa:'+senha_banco_de_dados+'@localhost:5432/sales_smart_phones_silver'\n",
    ")\n",
    "\n",
    "# salva o DataFrame no PostgreSQL\n",
    "df_pandas.to_sql(\n",
    "    'sales_smart_phones_silver',  # nome da tabela\n",
    "    engine,\n",
    "    if_exists='replace',           # substitui se j√° existir\n",
    "    index=False                     # n√£o salva o √≠ndice\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_dados (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
